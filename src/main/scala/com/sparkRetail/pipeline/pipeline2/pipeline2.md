Here is the **full `.md` file**, all sections included, clean, structured, and written exactly as you asked â€” **complete, ready to copy-paste**, fully inside one Markdown block.

---

````md
# ğŸ“˜ Pipeline Documentation (Complete)

This document contains:

- âœ” Input table creation SQL  
- âœ” Keyspaces table CQL  
- âœ” Directory structure in S3  
- âœ” Avro schema  
- âœ” Explanation of expected output  

---

## âœ” 1. Input Table Creation SQL (MySQL - RDS)

```sql
CREATE TABLE customers (
    customer_id INT PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(100),
    city VARCHAR(100)
);

CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    customer_id INT,
    order_date DATE,
    amount DOUBLE,
    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
);

CREATE TABLE order_items (
    item_id INT PRIMARY KEY,
    order_id INT,
    product_name VARCHAR(200),
    quantity INT,
    FOREIGN KEY (order_id) REFERENCES orders(order_id)
);
````

---

## âœ” 2. Amazon Keyspaces Table CQL

```cql
CREATE TABLE retail.sales_data (
    customer_id int,
    order_id int,
    item_id int,
    name text,
    email text,
    city text,
    order_date timestamp,
    amount double,
    product_name text,
    quantity int,
    PRIMARY KEY ((customer_id), order_id, item_id)
) WITH CLUSTERING ORDER BY (order_id ASC, item_id ASC);
```

---

## âœ” 3. Directory Structure Created in S3 (Pipeline 2 Output)

After writing partitioned Parquet files using:

```scala
.partitionBy("customer_id")
```

Spark automatically creates:

```
s3://spark-bucks/sales/parquet/
    â”œâ”€â”€ customer_id=1/
    â”œâ”€â”€ customer_id=2/
    â”œâ”€â”€ customer_id=3/
    â”œâ”€â”€ customer_id=4/
    â”œâ”€â”€ customer_id=5/
    â”œâ”€â”€ customer_id=6/
    â”œâ”€â”€ ...
    â””â”€â”€ customer_id=33/
```

Inside each folder:

```
part-0000x-*.snappy.parquet
SUCCESS
```

No need to manually create folders â€” Spark handles everything.

---

## âœ” 4. Avro Schema (for Kafka or Archival Use)

```json
{
  "type": "record",
  "name": "SalesData",
  "namespace": "com.retail.sales",
  "fields": [
    {"name": "customer_id", "type": "int"},
    {"name": "order_id", "type": "int"},
    {"name": "item_id", "type": "int"},
    {"name": "name", "type": "string"},
    {"name": "email", "type": "string"},
    {"name": "city", "type": "string"},
    {"name": "order_date", "type": {"type": "long", "logicalType": "timestamp-millis"}},
    {"name": "amount", "type": "double"},
    {"name": "product_name", "type": "string"},
    {"name": "quantity", "type": "int"}
  ]
}
```

---

## âœ” 5. Explanation of Expected Output

### ğŸŸ¦ Pipeline 1 (MySQL â†’ Spark â†’ Keyspaces)

**What happens:**

* Spark reads `customers`, `orders`, `order_items`
* Performs required joins
* Produces one denormalized dataframe containing:

  ```
  customer_id, name, email, city,
  order_id, order_date, amount,
  item_id, product_name, quantity
  ```
* Writes results into Keyspaces table `retail.sales_data`

**Expected output:**

* Keyspaces table fills with **33Ã—3-level clustered data**
* Duplicate `customer_id` values are completely normal
  (because multiple orders & items belong to the same customer)
* Data is stored efficiently using **(customer_id, order_id, item_id)** as composite key.

---

### ğŸŸ¦ Pipeline 2 (Keyspaces â†’ S3 Parquet)

**What happens:**

* Spark reads `sales_data` from Keyspaces
* Selects 5 fields:

```
customer_id
order_id
amount
product_name
quantity
```

* Writes Parquet files partitioned by `customer_id`

**Expected S3 output:**

Spark automatically generates folders:

```
customer_id=1/
customer_id=2/
customer_id=3/
...
```

Each folder contains:

* Parquet file with data only for that customer
* `SUCCESS` file indicating job completion

This structure is ideal for:

* Athena / Presto queries
* EMR / Glue ETL
* Fast analytical workloads

Because partition pruning reduces scan time.

---

## ğŸ¯ Final Summary

| Section                     | Status      |
| --------------------------- | ----------- |
| Input SQL                   | âœ” Completed |
| Keyspaces CQL               | âœ” Completed |
| S3 Folder Structure         | âœ” Completed |
| Avro Schema                 | âœ” Completed |
| Expected Output Explanation | âœ” Completed |

Everything is now documented cleanly in one `.md` file.

---

If you want this exported as an actual **.md file**, just say:
**â€œexport this as file.mdâ€**


![img.png](img.png)